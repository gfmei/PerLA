<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="PerLA: Perceptive 3D Language Assistant" name="description">
    <meta content="PerLA, 3D Language Assistant" name="keywords">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <title>PerLA</title>

    <link href="./static/css/bulma.min.css" rel="stylesheet">
    <link href="./static/css/bulma-carousel.min.css" rel="stylesheet">
    <link href="./static/css/bulma-slider.min.css" rel="stylesheet">
    <link href="./static/css/fontawesome.all.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
    <link href="./static/css/index.css" rel="stylesheet">
    <link href="./static/images/pearls.png" rel="icon" type="image/png">

    <!-- MathJax -->
    <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Title Section -->
                    <h1 class="title is-1 publication-title">
                        PerLA
                        <img alt="PerLA Logo" src="./static/images/pearls.png"
                             style="height: 1em; vertical-align: middle; margin: 0 0.3em;"/>
                        Perceptive 3D Language Assistant
                    </h1>

                    <!-- Authors Section -->
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=VsmIGqsAAAAJ"
                               target="_blank">Guofeng Mei</a>\(^{1}\),
                        </span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=JJRr8c8AAAAJ" target="_blank">Wei Lin</a>\(^2\),
                        </span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=djO2pVUAAAAJ"
                               target="_blank">Luigi Riz</a>\(^1\),
                        </span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=4t9fSdwAAAAJ"
                               target="_blank">Yujiao Wu</a>\(^3\),
                        </span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=BQ7li6AAAAAJ" target="_blank">Fabio Poiesi</a>\(^1\),
                        </span>
                        <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=KBZ3zrEAAAAJ"
                               target="_blank">Yiming Wang</a>\(^1\)
                        </span>
                    </div>

                    <!-- Affiliations Section -->
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">\(^{1}\) Fondazione Bruno Kessler, Italy;</span>
                        <span class="author-block">\(^{2}\) JKU Linz, Austria;</span>
                        <span class="author-block">\(^{3}\) CSIRO, Australia</span>
                    </div>

                    <!-- Links Section -->
                    <div class="publication-links">
                        <span class="link-block">
                            <a class="external-link button is-normal is-rounded is-dark"
                               href="https://arxiv.org/pdf/xxxxxxx.pdf" target="_blank">
                                <span class="icon" style="color: white;">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                <span>arXiv</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a class="external-link button is-normal is-rounded"
                               href="https://github.com/gfmei/PerLA" style="background-color: lightgray; color: black; border: none; padding: 10px; display: inline-flex; align-items: center;"
                               target="_blank">
                                <span class="icon" style="color: black;">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code (coming soon)</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Enabling Large Language Models (LLMs) to understand the 3D physical world is an emerging yet
                        challenging research direction.
                        Current strategies for processing point clouds typically downsample the scene or divide it into
                        smaller parts for separate analysis.
                        However, both approaches risk losing key local details or global contextual information.
                    </p>
                    <p>
                        This paper introduces PerLA, a 3D language assistant designed to be perceptive to both details
                        and context,
                        making visual representations more informative for the LLM.
                    </p>
                    <p><b>
                        PerLA captures high-resolution (local) details in parallel from different point cloud areas and
                        integrates
                        them with (global) context obtained from a lower-resolution whole point cloud.
                        We present a novel algorithm that preserves point cloud locality through the Hilbert curve and
                        effectively
                        aggregates local-to-global information via cross-attention and a graph neural network.
                    </b></p>
                    <p>
                        PerLA outperforms state-of-the-art 3D language assistants, with gains of up to +1.34 CiDEr on
                        ScanQA for question answering,
                        and +4.22 on ScanRefer and +3.88 on Nr3D for dense captioning.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<hr>

<!-- 3D Question Answering -->
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">3D Question Answering on ScanQA <small>[<a
                        href="https://github.com/ScanQA/ScanQA" target="_blank">1</a>]</small></h2>
                <img alt="3D Question Answering on ScanQA Results" src="./static/images/scanqa.png"/>
                <figcaption>
                    PerLA successfully identifies and reasons about objects and their relationships within the scene,
                    outperforming LL3DA [<a href="https://arxiv.org/pdf/2311.18651" target="_blank">2</a>].
                </figcaption>
            </div>
        </div>
    </div>
</section>

<hr>

<!-- 3D Dense Captioning: ScanRefer -->
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">3D Dense Captioning on ScanRefer <small>[<a
                        href="https://arxiv.org/abs/1912.08830" target="_blank">3</a>]</small></h2>
                <img alt="3D Dense Captioning on ScanRefer Results" src="./static/images/scanrefer.png"/>
                <figcaption>
                    PerLA demonstrates robust descriptive capabilities on ScanRefer, surpassing LL3DA [<a
                        href="https://arxiv.org/pdf/2311.18651" target="_blank">4</a>] by effectively capturing object
                    attributes
                    such as “the rectangular brown desk” and “the round table in the center of the room.”
                </figcaption>
            </div>
        </div>
    </div>
</section>

<hr>

<!-- 3D Dense Captioning: Nr3D -->
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">3D Dense Captioning on Nr3D <small>[<a
                        href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf"
                        target="_blank">5</a>]</small></h2>
                <img alt="3D Dense Captioning on Nr3D Results" src="./static/images/nr3d.png"/>
                <figcaption>
                    PerLA showcases fine-grained spatial reasoning on Nr3D by identifying intricate object relationships
                    within
                    complex scenes, outperforming LL3DA [<a href="https://arxiv.org/pdf/2311.18651"
                                                            target="_blank">6</a>].
                </figcaption>
            </div>
        </div>
    </div>
</section>


<hr>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>
            <code>
            @article{mei2024perla,
                title     = {PerLA: Perceptive 3D Language Assistant},
                author    = {Guofeng Mei and Wei Lin and Luigi Riz and Yujiao Wu and Fabio Poiesi and Yiming Wang},
                journal   = {arXiv preprint arXiv:xxxxx}, % Replace with actual arXiv ID
                year      = {2024}
            }
            </code>
        </pre>
    </div>
</section>


<!-- References -->
<section class="section">
    <div class="container">
        <h3 class="title is-5">References</h3>
        <ol>
            <li><a href="https://arxiv.org/abs/2301.12597" target="_blank">BLIP-2: Bootstrapping Language-Image
                Pre-training
                with Frozen Image Encoders and Large Language Models</a></li>
            <li><a href="https://github.com/ScanQA/ScanQA" target="_blank">ScanQA: 3D Question Answering Dataset and
                Benchmark</a></li>
            <li><a href="https://arxiv.org/pdf/2311.18651" target="_blank">LL3DA: Leveraging Language for 3D
                Applications</a></li>
            <li><a href="https://arxiv.org/abs/1912.08830" target="_blank">ScanRefer: 3D Object Localization and
                Description</a></li>
            <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf" target="_blank">
                ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes</a></li>
        </ol>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <!-- Link to Paper -->
            <a class="icon-link" href="https://arxiv.org/abs/xxxx" target="_blank" title="View Paper on arXiv">
                <i class="fas fa-file-pdf fa-2x"></i>
            </a>
            <!-- Link to GitHub -->
            <a class="icon-link" class="external-link" href="https://github.com/gfmei/PerLA" target="_blank"
               title="View Project on GitHub">
                <i class="fab fa-github fa-2x"></i>
            </a>
        </div>
        <div class="columns is-centered" style="margin-top: 20px;">
            <div class="column is-8">
                <div class="content has-text-centered">
                    <p>
                        This website is licensed under a
                        <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
                            Creative Commons Attribution-ShareAlike 4.0 International License
                        </a>.
                    </p>
                    <p>
                        Template adapted from
                        <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>


</body>

</html>